{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive gRNA Data Preparation Pipeline\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook implements a **rigorous, step-by-step data preparation pipeline** for gRNA classification, incorporating biological insights from [Cooper et al. 2022](https://rnajournal.cshlp.org/content/28/7/972.full.pdf).\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Load & Validate Raw Sequences** - Load canonical gRNA from FASTA files\n",
    "2. **Parse GTF & Identify gRNA Regions** - For proper negative sampling\n",
    "3. **Generate Length-Matched Negatives** - Multi-source, GTF-excluded\n",
    "4. **Comprehensive Feature Extraction** - 120 biologically-informed features\n",
    "5. **Quality Control & Validation** - Verify no data leakage\n",
    "6. **Train/Val/Test Split** - Stratified splitting with balancing\n",
    "7. **Export Datasets** - Save feature-rich datasets for modeling\n",
    "\n",
    "### üî¨ Key Biological Principles:\n",
    "- **Evidence-based initiation patterns**: AAAA (39.7%), GAAA (33%), AGAA (12.1%) - NOT just ATATA!\n",
    "- **Flexible anchor detection**: Position 4-6, length 8-12 nt, AC-rich/G-poor\n",
    "- **Molecular ruler hypothesis**: Init + anchor length conserved at 15-19 nt\n",
    "- **A-elevated guiding regions**: 46% A-content (vs 25-30% in non-gRNA)\n",
    "- **Terminal T**: 90% of gRNAs end with T (facilitates U-tail addition)\n",
    "\n",
    "### ‚ö†Ô∏è Critical Requirements:\n",
    "- **NO sequence length in features!** (causes artifact learning)\n",
    "- **Length-matched negatives** (KS test p>0.05)\n",
    "- **Exclude gRNA regions** (using GTF annotations)\n",
    "- **Balanced classes** in all splits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, Tuple, List, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Imports loaded successfully\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define File Paths\n",
    "\n",
    "**Input files:**\n",
    "- `mOs_gRNA_final.fasta`: Canonical gRNA sequences from Cooper 2022\n",
    "- `mOs_Cooper_minicircle.fasta`: Minicircle genomes for negative sampling\n",
    "- `mOs_gRNA_final.gtf`: gRNA coordinates for exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths for your environment!\n",
    "PROJECT_ROOT = Path.home() / 'projects' / 'grna-inspector'\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'gRNAs' / 'Cooper_2022'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed' / 'comprehensive_pipeline'\n",
    "PLOTS_DIR = DATA_DIR / 'plots' / 'data_prep'\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input files\n",
    "GRNA_FILE = RAW_DIR / 'mOs_gRNA_final.fasta'\n",
    "MINICIRCLE_FILE = RAW_DIR / 'mOs_Cooper_minicircle.fasta'\n",
    "GTF_FILE = RAW_DIR / 'mOs_gRNA_final.gtf'\n",
    "\n",
    "print(\"Checking input files...\")\n",
    "for filepath in [GRNA_FILE, MINICIRCLE_FILE, GTF_FILE]:\n",
    "    if filepath.exists():\n",
    "        print(f\"  ‚úì {filepath.name}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {filepath.name} - NOT FOUND!\")\n",
    "\n",
    "print(f\"\\nOutput directory: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Load & Validate Positive Sequences\n",
    "\n",
    "Load canonical gRNA sequences from FASTA file and perform initial validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 1: LOAD & VALIDATE POSITIVE SEQUENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load positive sequences\n",
    "positive_sequences = {}\n",
    "for record in SeqIO.parse(GRNA_FILE, \"fasta\"):\n",
    "    seq = str(record.seq).upper().replace('U', 'T')\n",
    "    positive_sequences[record.id] = seq\n",
    "\n",
    "print(f\"\\nLoaded {len(positive_sequences):,} canonical gRNA sequences\")\n",
    "\n",
    "# Calculate statistics\n",
    "lengths = [len(seq) for seq in positive_sequences.values()]\n",
    "sequences = list(positive_sequences.values())\n",
    "positive_lengths = lengths  # Save for later\n",
    "\n",
    "# Nucleotide composition\n",
    "at_contents = [(seq.count('A') + seq.count('T')) / len(seq) * 100 for seq in sequences]\n",
    "gc_contents = [(seq.count('G') + seq.count('C')) / len(seq) * 100 for seq in sequences]\n",
    "\n",
    "print(\"\\nüìä Sequence Statistics:\")\n",
    "print(f\"  Length range: {min(lengths)}-{max(lengths)} nt\")\n",
    "print(f\"  Mean length: {np.mean(lengths):.1f} ¬± {np.std(lengths):.1f} nt\")\n",
    "print(f\"  Median length: {np.median(lengths):.0f} nt\")\n",
    "\n",
    "print(\"\\nüß¨ Nucleotide Composition:\")\n",
    "print(f\"  Mean AT-content: {np.mean(at_contents):.1f}%\")\n",
    "print(f\"  Mean GC-content: {np.mean(gc_contents):.1f}%\")\n",
    "\n",
    "# Quality checks\n",
    "n_count = sum(1 for seq in sequences if 'N' in seq)\n",
    "unique_seqs = set(sequences)\n",
    "n_duplicates = len(sequences) - len(unique_seqs)\n",
    "\n",
    "print(\"\\nüîç Quality Checks:\")\n",
    "print(f\"  Sequences with N: {n_count} ({n_count/len(sequences)*100:.1f}%)\")\n",
    "print(f\"  Duplicate sequences: {n_duplicates}\")\n",
    "\n",
    "if n_duplicates > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è Found {n_duplicates} duplicated sequence(s)\")\n",
    "    print(\"     (This is OK if same gRNA is encoded on multiple minicircles)\")\n",
    "\n",
    "print(\"\\n‚úÖ Positive sequences loaded and validated!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Parse GTF and Identify gRNA Regions\n",
    "\n",
    "Parse GTF file to get gRNA coordinates on each minicircle.\n",
    "This allows us to **exclude these regions** when generating negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gtf_file(gtf_file: Path) -> Dict[str, List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Parse GTF file and extract gRNA coordinates for each minicircle.\n",
    "    Coordinates are converted to 0-indexed, end-exclusive (Python convention).\n",
    "    \"\"\"\n",
    "    grna_regions = defaultdict(list)\n",
    "    \n",
    "    with open(gtf_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            minicircle_id = parts[0]\n",
    "            start = int(parts[3]) - 1  # Convert to 0-indexed\n",
    "            end = int(parts[4])  # Keep end as Python end-exclusive\n",
    "            grna_regions[minicircle_id].append((start, end))\n",
    "    \n",
    "    # Sort regions by start position\n",
    "    for mini_id in grna_regions:\n",
    "        grna_regions[mini_id].sort()\n",
    "    \n",
    "    return dict(grna_regions)\n",
    "\n",
    "\n",
    "def merge_overlapping_regions(regions: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Merge overlapping regions.\"\"\"\n",
    "    if not regions:\n",
    "        return []\n",
    "    merged = [regions[0]]\n",
    "    for start, end in regions[1:]:\n",
    "        last_start, last_end = merged[-1]\n",
    "        if start <= last_end:  # Overlapping or adjacent\n",
    "            merged[-1] = (last_start, max(last_end, end))\n",
    "        else:\n",
    "            merged.append((start, end))\n",
    "    return merged\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STAGE 2: PARSE GTF AND IDENTIFY gRNA REGIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "grna_regions = parse_gtf_file(GTF_FILE)\n",
    "\n",
    "print(f\"\\nüîç Parsed GTF file:\")\n",
    "print(f\"  Found gRNA annotations for {len(grna_regions)} minicircles\")\n",
    "\n",
    "# Merge overlapping regions\n",
    "total_before = sum(len(regions) for regions in grna_regions.values())\n",
    "for mini_id in grna_regions:\n",
    "    grna_regions[mini_id] = merge_overlapping_regions(grna_regions[mini_id])\n",
    "total_after = sum(len(regions) for regions in grna_regions.values())\n",
    "\n",
    "print(f\"\\n  Total gRNA annotations: {total_before}\")\n",
    "print(f\"  After merging overlaps: {total_after}\")\n",
    "\n",
    "# Show example\n",
    "example_mini = list(grna_regions.keys())[0]\n",
    "print(f\"\\n  Example (regions on {example_mini}):\")\n",
    "for start, end in grna_regions[example_mini][:3]:\n",
    "    print(f\"    {start:4d} - {end:4d} ({end-start} nt)\")\n",
    "\n",
    "print(\"\\n‚úÖ gRNA regions identified and ready for exclusion!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Generate Length-Matched Negative Examples\n",
    "\n",
    "### ‚ö†Ô∏è Critical: Avoiding Length Artifacts!\n",
    "\n",
    "**Multi-source approach:**\n",
    "1. **Minicircle non-gRNA regions** (50%): Sample from inter-gRNA regions\n",
    "2. **Chimeric sequences** (30%): Combine fragments from different minicircles\n",
    "3. **Composition-matched random** (20%): Generate with correct nucleotide frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_grna_regions(minicircle_id: str, minicircle_length: int,\n",
    "                         grna_coords: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Calculate non-gRNA regions (inter-gRNA spaces).\"\"\"\n",
    "    if not grna_coords:\n",
    "        return [(0, minicircle_length)]\n",
    "    \n",
    "    non_grna = []\n",
    "    # Before first gRNA\n",
    "    if grna_coords[0][0] > 0:\n",
    "        non_grna.append((0, grna_coords[0][0]))\n",
    "    # Between gRNAs\n",
    "    for i in range(len(grna_coords) - 1):\n",
    "        gap_start = grna_coords[i][1]\n",
    "        gap_end = grna_coords[i+1][0]\n",
    "        if gap_end > gap_start:\n",
    "            non_grna.append((gap_start, gap_end))\n",
    "    # After last gRNA\n",
    "    if grna_coords[-1][1] < minicircle_length:\n",
    "        non_grna.append((grna_coords[-1][1], minicircle_length))\n",
    "    return non_grna\n",
    "\n",
    "\n",
    "def generate_minicircle_negatives(minicircle_file: Path, \n",
    "                                   grna_regions_dict: Dict,\n",
    "                                   target_lengths: List[int],\n",
    "                                   n_samples: int) -> Dict[str, str]:\n",
    "    \"\"\"Generate negatives from minicircles, EXCLUDING gRNA regions.\"\"\"\n",
    "    # Load minicircles and calculate non-gRNA regions\n",
    "    minicircles = []\n",
    "    non_grna_regions = {}\n",
    "    \n",
    "    for record in SeqIO.parse(minicircle_file, \"fasta\"):\n",
    "        mini_id = record.id\n",
    "        seq = str(record.seq).upper().replace('U', 'T')\n",
    "        minicircles.append((mini_id, seq))\n",
    "        grna_coords = grna_regions_dict.get(mini_id, [])\n",
    "        non_grna = get_non_grna_regions(mini_id, len(seq), grna_coords)\n",
    "        non_grna_regions[mini_id] = non_grna\n",
    "    \n",
    "    print(f\"  Loaded {len(minicircles)} minicircles\")\n",
    "    \n",
    "    negatives = {}\n",
    "    attempts = 0\n",
    "    max_attempts = n_samples * 20\n",
    "    \n",
    "    while len(negatives) < n_samples and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        target_len = np.random.choice(target_lengths)\n",
    "        mini_id, mini_seq = minicircles[np.random.randint(len(minicircles))]\n",
    "        available_regions = non_grna_regions[mini_id]\n",
    "        \n",
    "        if not available_regions:\n",
    "            continue\n",
    "        \n",
    "        region = available_regions[np.random.randint(len(available_regions))]\n",
    "        region_start, region_end = region\n",
    "        region_len = region_end - region_start\n",
    "        \n",
    "        if region_len < target_len:\n",
    "            continue\n",
    "        \n",
    "        frag_start = np.random.randint(region_start, region_end - target_len + 1)\n",
    "        fragment = mini_seq[frag_start:frag_start + target_len]\n",
    "        \n",
    "        # Quality filters\n",
    "        if 'N' in fragment:\n",
    "            continue\n",
    "        if len(set(fragment)) == 1:\n",
    "            continue\n",
    "        \n",
    "        neg_id = f\"{mini_id}_nonGRNA_{frag_start}_{frag_start+target_len}\"\n",
    "        negatives[neg_id] = fragment\n",
    "    \n",
    "    return negatives\n",
    "\n",
    "\n",
    "def generate_chimeric_negatives(minicircle_file: Path,\n",
    "                                 grna_regions_dict: Dict,\n",
    "                                 target_lengths: List[int],\n",
    "                                 n_samples: int) -> Dict[str, str]:\n",
    "    \"\"\"Generate chimeric sequences by combining fragments from different minicircles.\"\"\"\n",
    "    minicircles = []\n",
    "    non_grna_regions = {}\n",
    "    \n",
    "    for record in SeqIO.parse(minicircle_file, \"fasta\"):\n",
    "        mini_id = record.id\n",
    "        seq = str(record.seq).upper().replace('U', 'T')\n",
    "        minicircles.append((mini_id, seq))\n",
    "        grna_coords = grna_regions_dict.get(mini_id, [])\n",
    "        non_grna = get_non_grna_regions(mini_id, len(seq), grna_coords)\n",
    "        non_grna_regions[mini_id] = non_grna\n",
    "    \n",
    "    chimeric = {}\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        target_len = np.random.choice(target_lengths)\n",
    "        n_fragments = np.random.randint(2, 5)\n",
    "        frag_lens = np.random.multinomial(target_len - n_fragments, \n",
    "                                           np.ones(n_fragments) / n_fragments) + 1\n",
    "        \n",
    "        fragments = []\n",
    "        for frag_len in frag_lens:\n",
    "            for _ in range(100):\n",
    "                mini_id, mini_seq = minicircles[np.random.randint(len(minicircles))]\n",
    "                available = non_grna_regions[mini_id]\n",
    "                if not available:\n",
    "                    continue\n",
    "                region = available[np.random.randint(len(available))]\n",
    "                if region[1] - region[0] >= frag_len:\n",
    "                    start = np.random.randint(region[0], region[1] - frag_len + 1)\n",
    "                    fragments.append(mini_seq[start:start + frag_len])\n",
    "                    break\n",
    "        \n",
    "        if len(fragments) == n_fragments:\n",
    "            combined = ''.join(fragments)\n",
    "            if 'N' not in combined and len(set(combined)) > 1:\n",
    "                chimeric[f'chimeric_{i}'] = combined\n",
    "    \n",
    "    return chimeric\n",
    "\n",
    "\n",
    "def generate_random_negatives(positive_sequences: Dict[str, str],\n",
    "                               target_lengths: List[int],\n",
    "                               n_samples: int) -> Dict[str, str]:\n",
    "    \"\"\"Generate random sequences matching nucleotide composition.\"\"\"\n",
    "    # Calculate overall composition from positives\n",
    "    all_seqs = ''.join(positive_sequences.values())\n",
    "    freqs = {nt: all_seqs.count(nt) / len(all_seqs) for nt in 'ATGC'}\n",
    "    nucs = list(freqs.keys())\n",
    "    probs = list(freqs.values())\n",
    "    \n",
    "    randoms = {}\n",
    "    for i in range(n_samples):\n",
    "        length = np.random.choice(target_lengths)\n",
    "        seq = ''.join(np.random.choice(nucs, size=length, p=probs))\n",
    "        randoms[f'random_{i}'] = seq\n",
    "    \n",
    "    return randoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 3: GENERATE LENGTH-MATCHED NEGATIVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_positives = len(positive_sequences)\n",
    "n_total_negatives = n_positives  # 1:1 ratio\n",
    "\n",
    "# Distribution: 50% minicircle, 30% chimeric, 20% random\n",
    "n_minicircle = int(n_total_negatives * 0.50)\n",
    "n_chimeric = int(n_total_negatives * 0.30)\n",
    "n_random = n_total_negatives - n_minicircle - n_chimeric\n",
    "\n",
    "print(f\"\\nüìä Negative sampling strategy:\")\n",
    "print(f\"  Minicircle non-gRNA: {n_minicircle} (50%)\")\n",
    "print(f\"  Chimeric: {n_chimeric} (30%)\")\n",
    "print(f\"  Random: {n_random} (20%)\")\n",
    "print(f\"  Total: {n_total_negatives}\")\n",
    "\n",
    "# Generate negatives\n",
    "print(\"\\n[1/3] Generating minicircle non-gRNA negatives...\")\n",
    "minicircle_negatives = generate_minicircle_negatives(\n",
    "    MINICIRCLE_FILE, grna_regions, positive_lengths, n_minicircle\n",
    ")\n",
    "print(f\"  Generated: {len(minicircle_negatives)}\")\n",
    "\n",
    "print(\"\\n[2/3] Generating chimeric negatives...\")\n",
    "chimeric_negatives = generate_chimeric_negatives(\n",
    "    MINICIRCLE_FILE, grna_regions, positive_lengths, n_chimeric\n",
    ")\n",
    "print(f\"  Generated: {len(chimeric_negatives)}\")\n",
    "\n",
    "print(\"\\n[3/3] Generating random negatives...\")\n",
    "random_negatives = generate_random_negatives(\n",
    "    positive_sequences, positive_lengths, n_random\n",
    ")\n",
    "print(f\"  Generated: {len(random_negatives)}\")\n",
    "\n",
    "# Combine all negatives\n",
    "all_negatives = {**minicircle_negatives, **chimeric_negatives, **random_negatives}\n",
    "negative_lengths = [len(seq) for seq in all_negatives.values()]\n",
    "\n",
    "print(f\"\\n‚úÖ Total negatives generated: {len(all_negatives):,}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Length Matching (KS Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KS test\n",
    "ks_stat, ks_pval = stats.ks_2samp(positive_lengths, negative_lengths)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LENGTH DISTRIBUTION VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìà Kolmogorov-Smirnov Test:\")\n",
    "print(f\"  KS statistic: {ks_stat:.4f}\")\n",
    "print(f\"  p-value: {ks_pval:.4f}\")\n",
    "\n",
    "if ks_pval > 0.05:\n",
    "    print(f\"\\n  ‚úÖ PASS: Distributions are statistically identical (p={ks_pval:.4f})\")\n",
    "    print(f\"     ‚Üí No length leakage!\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ùå FAIL: Distributions differ (p={ks_pval:.4f})\")\n",
    "    print(f\"     ‚Üí WARNING: May need to regenerate!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histograms\n",
    "axes[0].hist(positive_lengths, bins=30, alpha=0.5, label='Positive', \n",
    "            color='steelblue', edgecolor='black')\n",
    "axes[0].hist(negative_lengths, bins=30, alpha=0.5, label='Negative', \n",
    "            color='coral', edgecolor='black')\n",
    "axes[0].set_xlabel('Sequence Length (nt)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Length Distribution Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# ECDF\n",
    "pos_sorted = np.sort(positive_lengths)\n",
    "neg_sorted = np.sort(negative_lengths)\n",
    "pos_ecdf = np.arange(1, len(pos_sorted)+1) / len(pos_sorted)\n",
    "neg_ecdf = np.arange(1, len(neg_sorted)+1) / len(neg_sorted)\n",
    "\n",
    "axes[1].plot(pos_sorted, pos_ecdf, label='Positive', color='steelblue')\n",
    "axes[1].plot(neg_sorted, neg_ecdf, label='Negative', color='coral')\n",
    "axes[1].set_xlabel('Sequence Length (nt)')\n",
    "axes[1].set_ylabel('ECDF')\n",
    "axes[1].set_title(f'ECDF (KS p={ks_pval:.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'length_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Plot saved\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Comprehensive Feature Extraction\n",
    "\n",
    "Extract **120 biologically-informed features** from each sequence:\n",
    "- Evidence-based initiation patterns (AAAA, GAAA, AGAA - NOT just ATATA!)\n",
    "- Flexible anchor region detection\n",
    "- Guiding region composition\n",
    "- Terminal features\n",
    "- K-mer and structural features\n",
    "\n",
    "**CRITICAL: NO LENGTH FEATURES!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGrnaFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced feature extractor based on empirical analysis of Cooper et al. 2022 data.\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Multiple initiation patterns (AAAA 39.7%, GAAA 33%, AGAA 12.1% - NOT just ATATA!)\n",
    "    2. Flexible anchor region detection (position 4-6, length 8-12)\n",
    "    3. Evidence-based thresholds from real data\n",
    "    4. NO LENGTH FEATURES (critical for avoiding artifacts!)\n",
    "    \n",
    "    Total features: 120\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # EVIDENCE-BASED initiation patterns\n",
    "        self.initiation_patterns = {\n",
    "            'ATATA': 'ATATA',\n",
    "            'AWAHH': r'A[AT]A[ACT][ACT]',\n",
    "            'ATRTR': r'AT[AG]T[AG]',\n",
    "            'AWAWA': r'A[AT]A[AT]A',\n",
    "            'AAAA': 'AAAA',        # 39.7% in real data!\n",
    "            'GAAA': 'GAAA',        # 33.0% in real data!\n",
    "            'AGAA': 'AGAA',        # 12.1% in real data!\n",
    "            'TAAA': 'TAAA',\n",
    "            'CAAA': 'CAAA',\n",
    "            'XAAA': r'[ATGC]AAA',\n",
    "            'AXAA': r'A[ATGC]AA',\n",
    "        }\n",
    "        self.important_3mers = ['AAA', 'ATA', 'TAT', 'TTT', 'AAT', 'ATT', 'GAA', 'AGA']\n",
    "        self.important_4mers = ['ATAT', 'TATA', 'AAAA', 'TTTT', 'AAAG', 'AAGA', 'GAAA', 'AGAA']\n",
    "        self.anchor_start_range = (4, 6)\n",
    "        self.anchor_length_range = (8, 12)\n",
    "    \n",
    "    def extract_features(self, sequence):\n",
    "        features = {}\n",
    "        seq = sequence.upper().replace('U', 'T')\n",
    "        \n",
    "        features.update(self._extract_initiation_features(seq))\n",
    "        features.update(self._extract_anchor_features(seq))\n",
    "        features.update(self._extract_guiding_features(seq))\n",
    "        features.update(self._extract_terminal_features(seq))\n",
    "        features.update(self._extract_kmer_features(seq))\n",
    "        features.update(self._extract_structural_features(seq))\n",
    "        features.update(self._extract_positional_features(seq))\n",
    "        features.update(self._extract_dinucleotide_features(seq))\n",
    "        features.update(self._extract_composition_features(seq))\n",
    "        features.update(self._extract_advanced_features(seq))\n",
    "        features.update(self._extract_meta_features(seq, features))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_initiation_features(self, seq):\n",
    "        features = {}\n",
    "        init_region = seq[:6] if len(seq) >= 6 else seq\n",
    "        \n",
    "        for pattern_name, pattern in self.initiation_patterns.items():\n",
    "            has_pattern = bool(re.match(pattern, init_region))\n",
    "            features[f'init_has_{pattern_name}'] = float(has_pattern)\n",
    "        \n",
    "        features['init_starts_A'] = float(seq[0] == 'A') if len(seq) > 0 else 0.0\n",
    "        features['init_starts_G'] = float(seq[0] == 'G') if len(seq) > 0 else 0.0\n",
    "        features['init_starts_T'] = float(seq[0] == 'T') if len(seq) > 0 else 0.0\n",
    "        features['init_starts_C'] = float(seq[0] == 'C') if len(seq) > 0 else 0.0\n",
    "        features['init_starts_purine'] = float(seq[0] in 'AG') if len(seq) > 0 else 0.0\n",
    "        \n",
    "        first4 = seq[:4] if len(seq) >= 4 else seq\n",
    "        if len(first4) > 0:\n",
    "            features['init_4_A_count'] = first4.count('A')\n",
    "            features['init_4_T_count'] = first4.count('T')\n",
    "            features['init_4_G_count'] = first4.count('G')\n",
    "            features['init_4_C_count'] = first4.count('C')\n",
    "            features['init_4_A_rich'] = float(first4.count('A') >= 3)\n",
    "        else:\n",
    "            for f in ['init_4_A_count', 'init_4_T_count', 'init_4_G_count', 'init_4_C_count', 'init_4_A_rich']:\n",
    "                features[f] = 0.0\n",
    "        \n",
    "        total_patterns = sum(1 for p in self.initiation_patterns \n",
    "                           if features.get(f'init_has_{p}', 0) > 0)\n",
    "        features['init_pattern_count'] = float(total_patterns)\n",
    "        features['init_any_known_pattern'] = float(total_patterns > 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_anchor_features(self, seq):\n",
    "        features = {}\n",
    "        best_anchor = None\n",
    "        best_score = -1\n",
    "        best_start = 0\n",
    "        \n",
    "        for start in range(self.anchor_start_range[0], \n",
    "                          min(self.anchor_start_range[1] + 1, len(seq) - 5)):\n",
    "            for length in range(self.anchor_length_range[0], \n",
    "                               min(self.anchor_length_range[1] + 1, len(seq) - start + 1)):\n",
    "                anchor = seq[start:start + length]\n",
    "                if len(anchor) < 5:\n",
    "                    continue\n",
    "                ac_content = (anchor.count('A') + anchor.count('C')) / len(anchor)\n",
    "                g_content = anchor.count('G') / len(anchor)\n",
    "                score = ac_content - g_content\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_anchor = anchor\n",
    "                    best_start = start\n",
    "        \n",
    "        if best_anchor and len(best_anchor) > 0:\n",
    "            anchor = best_anchor\n",
    "            for nt in 'ATGC':\n",
    "                features[f'anchor_{nt}_freq'] = anchor.count(nt) / len(anchor)\n",
    "            features['anchor_AT_freq'] = (anchor.count('A') + anchor.count('T')) / len(anchor)\n",
    "            features['anchor_GC_freq'] = (anchor.count('G') + anchor.count('C')) / len(anchor)\n",
    "            features['anchor_purine_freq'] = (anchor.count('A') + anchor.count('G')) / len(anchor)\n",
    "            features['anchor_AC_content'] = (anchor.count('A') + anchor.count('C')) / len(anchor)\n",
    "            features['anchor_length'] = float(len(anchor))\n",
    "            features['anchor_start_pos'] = float(best_start)\n",
    "            features['anchor_G_depleted'] = float(features['anchor_G_freq'] < 0.15)\n",
    "            features['anchor_AC_rich'] = float(features['anchor_AC_content'] > 0.60)\n",
    "            features['anchor_AC_very_rich'] = float(features['anchor_AC_content'] > 0.70)\n",
    "            init_anchor_len = best_start + len(anchor)\n",
    "            features['init_anchor_total_len'] = float(init_anchor_len)\n",
    "            features['in_molecular_ruler_range'] = float(15 <= init_anchor_len <= 19)\n",
    "            features['anchor_entropy'] = self._calculate_entropy(anchor)\n",
    "            features['anchor_unique_dinucs'] = float(len(set(\n",
    "                anchor[i:i+2] for i in range(len(anchor)-1)\n",
    "            ))) if len(anchor) > 1 else 0.0\n",
    "        else:\n",
    "            for ft in ['anchor_A_freq', 'anchor_T_freq', 'anchor_G_freq', 'anchor_C_freq',\n",
    "                      'anchor_AT_freq', 'anchor_GC_freq', 'anchor_purine_freq', 'anchor_AC_content',\n",
    "                      'anchor_length', 'anchor_start_pos', 'anchor_G_depleted', 'anchor_AC_rich',\n",
    "                      'anchor_AC_very_rich', 'init_anchor_total_len', 'in_molecular_ruler_range',\n",
    "                      'anchor_entropy', 'anchor_unique_dinucs']:\n",
    "                features[ft] = 0.0\n",
    "        return features\n",
    "    \n",
    "    def _extract_guiding_features(self, seq):\n",
    "        features = {}\n",
    "        guide_start = min(15, len(seq))\n",
    "        guide = seq[guide_start:]\n",
    "        \n",
    "        if len(guide) > 0:\n",
    "            for nt in 'ATGC':\n",
    "                features[f'guide_{nt}_freq'] = guide.count(nt) / len(guide)\n",
    "            features['guide_AT_freq'] = (guide.count('A') + guide.count('T')) / len(guide)\n",
    "            features['guide_GC_freq'] = (guide.count('G') + guide.count('C')) / len(guide)\n",
    "            features['guide_A_elevated'] = float(features['guide_A_freq'] > 0.40)\n",
    "            features['guide_A_content_high'] = float(features['guide_A_freq'] > 0.45)\n",
    "            purine_freq = (guide.count('A') + guide.count('G')) / len(guide)\n",
    "            features['guide_purine_freq'] = purine_freq\n",
    "            features['guide_purine_rich'] = float(purine_freq > 0.55)\n",
    "            features['guide_pyrimidine_freq'] = (guide.count('T') + guide.count('C')) / len(guide)\n",
    "            features['guide_C_count'] = float(guide.count('C'))\n",
    "            features['guide_T_count'] = float(guide.count('T'))\n",
    "            features['guide_edit_potential'] = (guide.count('C') + guide.count('T')) / len(guide)\n",
    "        else:\n",
    "            for ft in ['guide_A_freq', 'guide_T_freq', 'guide_G_freq', 'guide_C_freq',\n",
    "                      'guide_AT_freq', 'guide_GC_freq', 'guide_A_elevated', 'guide_A_content_high',\n",
    "                      'guide_purine_freq', 'guide_purine_rich', 'guide_pyrimidine_freq',\n",
    "                      'guide_C_count', 'guide_T_count', 'guide_edit_potential']:\n",
    "                features[ft] = 0.0\n",
    "        return features\n",
    "    \n",
    "    def _extract_terminal_features(self, seq):\n",
    "        features = {}\n",
    "        if len(seq) > 0:\n",
    "            features['ends_with_T'] = float(seq[-1] == 'T')\n",
    "            features['ends_with_A'] = float(seq[-1] == 'A')\n",
    "            features['ends_with_G'] = float(seq[-1] == 'G')\n",
    "            features['ends_with_C'] = float(seq[-1] == 'C')\n",
    "            last3 = seq[-3:] if len(seq) >= 3 else seq\n",
    "            features['last3_T_count'] = float(last3.count('T'))\n",
    "            features['last3_A_count'] = float(last3.count('A'))\n",
    "            features['last3_TT'] = float(last3.endswith('TT')) if len(last3) >= 2 else 0.0\n",
    "            features['last3_AT'] = float('AT' in last3) if len(last3) >= 2 else 0.0\n",
    "            last5 = seq[-5:] if len(seq) >= 5 else seq\n",
    "            if len(last5) > 0:\n",
    "                features['last5_T_freq'] = last5.count('T') / len(last5)\n",
    "                features['last5_A_freq'] = last5.count('A') / len(last5)\n",
    "                features['last5_AT_freq'] = (last5.count('A') + last5.count('T')) / len(last5)\n",
    "            else:\n",
    "                features['last5_T_freq'] = features['last5_A_freq'] = features['last5_AT_freq'] = 0.0\n",
    "            features['ends_poly_T_2'] = float(seq[-2:] == 'TT') if len(seq) >= 2 else 0.0\n",
    "            features['ends_poly_T_3'] = float(seq[-3:] == 'TTT') if len(seq) >= 3 else 0.0\n",
    "        else:\n",
    "            for ft in ['ends_with_T', 'ends_with_A', 'ends_with_G', 'ends_with_C',\n",
    "                      'last3_T_count', 'last3_A_count', 'last3_TT', 'last3_AT',\n",
    "                      'last5_T_freq', 'last5_A_freq', 'last5_AT_freq',\n",
    "                      'ends_poly_T_2', 'ends_poly_T_3']:\n",
    "                features[ft] = 0.0\n",
    "        return features\n",
    "    \n",
    "    def _extract_kmer_features(self, seq):\n",
    "        features = {}\n",
    "        n = len(seq)\n",
    "        if n < 3:\n",
    "            for kmer in self.important_3mers:\n",
    "                features[f'kmer3_{kmer}_count'] = 0.0\n",
    "                features[f'kmer3_{kmer}_freq'] = 0.0\n",
    "            for kmer in self.important_4mers:\n",
    "                features[f'kmer4_{kmer}_present'] = 0.0\n",
    "            return features\n",
    "        \n",
    "        kmer3_counts = Counter(seq[i:i+3] for i in range(n-2))\n",
    "        total_3mers = n - 2\n",
    "        for kmer in self.important_3mers:\n",
    "            count = kmer3_counts.get(kmer, 0)\n",
    "            features[f'kmer3_{kmer}_count'] = float(count)\n",
    "            features[f'kmer3_{kmer}_freq'] = count / total_3mers if total_3mers > 0 else 0.0\n",
    "        \n",
    "        if n >= 4:\n",
    "            for kmer in self.important_4mers:\n",
    "                features[f'kmer4_{kmer}_present'] = float(kmer in seq)\n",
    "        else:\n",
    "            for kmer in self.important_4mers:\n",
    "                features[f'kmer4_{kmer}_present'] = 0.0\n",
    "        return features\n",
    "    \n",
    "    def _extract_structural_features(self, seq):\n",
    "        features = {}\n",
    "        n = len(seq)\n",
    "        if n == 0:\n",
    "            features['entropy'] = 0.0\n",
    "            features['complexity_ratio'] = 0.0\n",
    "            features['max_homopolymer'] = 0.0\n",
    "            features['n_homopolymers_3plus'] = 0.0\n",
    "            return features\n",
    "        \n",
    "        features['entropy'] = self._calculate_entropy(seq)\n",
    "        if n >= 3:\n",
    "            unique_3mers = len(set(seq[i:i+3] for i in range(n-2)))\n",
    "            possible_3mers = min(n - 2, 64)\n",
    "            features['complexity_ratio'] = unique_3mers / possible_3mers if possible_3mers > 0 else 0.0\n",
    "        else:\n",
    "            features['complexity_ratio'] = 0.0\n",
    "        \n",
    "        max_run = 0\n",
    "        n_runs = 0\n",
    "        current_run = 1\n",
    "        for i in range(1, n):\n",
    "            if seq[i] == seq[i-1]:\n",
    "                current_run += 1\n",
    "            else:\n",
    "                if current_run >= 3:\n",
    "                    n_runs += 1\n",
    "                max_run = max(max_run, current_run)\n",
    "                current_run = 1\n",
    "        if current_run >= 3:\n",
    "            n_runs += 1\n",
    "        max_run = max(max_run, current_run)\n",
    "        features['max_homopolymer'] = float(max_run)\n",
    "        features['n_homopolymers_3plus'] = float(n_runs)\n",
    "        return features\n",
    "    \n",
    "    def _extract_positional_features(self, seq):\n",
    "        features = {}\n",
    "        n = len(seq)\n",
    "        if n == 0:\n",
    "            features['first_A_pos_rel'] = 0.0\n",
    "            features['first_G_pos_rel'] = 0.0\n",
    "            features['last_T_pos_rel'] = 0.0\n",
    "            return features\n",
    "        for nt in ['A', 'G']:\n",
    "            pos = seq.find(nt)\n",
    "            features[f'first_{nt}_pos_rel'] = pos / n if pos >= 0 else 1.0\n",
    "        for nt in ['T']:\n",
    "            pos = seq.rfind(nt)\n",
    "            features[f'last_{nt}_pos_rel'] = pos / n if pos >= 0 else 0.0\n",
    "        return features\n",
    "    \n",
    "    def _extract_dinucleotide_features(self, seq):\n",
    "        features = {}\n",
    "        n = len(seq)\n",
    "        important_dinucs = ['AA', 'AT', 'TA', 'TT', 'GC', 'CG', 'AC', 'CA']\n",
    "        if n < 2:\n",
    "            for dn in important_dinucs:\n",
    "                features[f'dinuc_{dn}_freq'] = 0.0\n",
    "            features['dinuc_bias_AT'] = 0.0\n",
    "            return features\n",
    "        \n",
    "        dinuc_counts = Counter(seq[i:i+2] for i in range(n-1))\n",
    "        total_dinucs = n - 1\n",
    "        for dn in important_dinucs:\n",
    "            features[f'dinuc_{dn}_freq'] = dinuc_counts.get(dn, 0) / total_dinucs\n",
    "        at_dinucs = sum(dinuc_counts.get(d, 0) for d in ['AA', 'AT', 'TA', 'TT'])\n",
    "        features['dinuc_bias_AT'] = at_dinucs / total_dinucs\n",
    "        return features\n",
    "    \n",
    "    def _extract_composition_features(self, seq):\n",
    "        features = {}\n",
    "        n = len(seq)\n",
    "        if n == 0:\n",
    "            for nt in 'ATGC':\n",
    "                features[f'global_{nt}_freq'] = 0.0\n",
    "            features['global_AT_content'] = 0.0\n",
    "            features['global_GC_content'] = 0.0\n",
    "            features['global_purine_content'] = 0.0\n",
    "            return features\n",
    "        for nt in 'ATGC':\n",
    "            features[f'global_{nt}_freq'] = seq.count(nt) / n\n",
    "        features['global_AT_content'] = (seq.count('A') + seq.count('T')) / n\n",
    "        features['global_GC_content'] = (seq.count('G') + seq.count('C')) / n\n",
    "        features['global_purine_content'] = (seq.count('A') + seq.count('G')) / n\n",
    "        return features\n",
    "    \n",
    "    def _extract_advanced_features(self, seq):\n",
    "        features = {}\n",
    "        n = len(seq)\n",
    "        if n == 0:\n",
    "            features['skew_AT'] = 0.0\n",
    "            features['skew_GC'] = 0.0\n",
    "            features['balance_ratio'] = 0.0\n",
    "            return features\n",
    "        a, t, g, c = seq.count('A'), seq.count('T'), seq.count('G'), seq.count('C')\n",
    "        features['skew_AT'] = (a - t) / (a + t) if a + t > 0 else 0.0\n",
    "        features['skew_GC'] = (g - c) / (g + c) if g + c > 0 else 0.0\n",
    "        freqs = [a, t, g, c]\n",
    "        features['balance_ratio'] = min(freqs) / max(freqs) if max(freqs) > 0 else 0.0\n",
    "        return features\n",
    "    \n",
    "    def _extract_meta_features(self, seq, existing_features):\n",
    "        features = {}\n",
    "        init_score = existing_features.get('init_any_known_pattern', 0)\n",
    "        anchor_score = existing_features.get('anchor_AC_rich', 0)\n",
    "        guide_score = existing_features.get('guide_A_elevated', 0)\n",
    "        terminal_score = existing_features.get('ends_with_T', 0)\n",
    "        features['grna_signature_count'] = init_score + anchor_score + guide_score + terminal_score\n",
    "        features['grna_signature_all'] = float(\n",
    "            init_score > 0 and anchor_score > 0 and guide_score > 0 and terminal_score > 0\n",
    "        )\n",
    "        features['init_anchor_quality'] = (\n",
    "            existing_features.get('init_any_known_pattern', 0) * 0.3 +\n",
    "            existing_features.get('anchor_AC_rich', 0) * 0.4 +\n",
    "            existing_features.get('in_molecular_ruler_range', 0) * 0.3\n",
    "        )\n",
    "        return features\n",
    "    \n",
    "    def _calculate_entropy(self, seq):\n",
    "        if len(seq) == 0:\n",
    "            return 0.0\n",
    "        counts = Counter(seq)\n",
    "        probs = [count / len(seq) for count in counts.values()]\n",
    "        return -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        dummy = self.extract_features('AAAAGCACTTTAAATTGCGCGCGCGCGCGCGCGT')\n",
    "        return list(dummy.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 4: FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "extractor = EnhancedGrnaFeatureExtractor()\n",
    "print(f\"\\n‚úì Feature extractor initialized\")\n",
    "print(f\"  Total features: {len(extractor.get_feature_names())}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"Extracting features from all sequences...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "# Add positives\n",
    "print(f\"\\n[1/2] Processing {len(positive_sequences):,} positive sequences...\")\n",
    "for i, (seq_id, seq) in enumerate(positive_sequences.items()):\n",
    "    if i % 300 == 0:\n",
    "        print(f\"  Progress: {i}/{len(positive_sequences)}\")\n",
    "    features = extractor.extract_features(seq)\n",
    "    row = {\n",
    "        'sequence_id': seq_id,\n",
    "        'sequence': seq,\n",
    "        'length': len(seq),  # Stored but NOT used as feature!\n",
    "        'label': 1,\n",
    "        'source': 'canonical_gRNA',\n",
    "        **features\n",
    "    }\n",
    "    data_rows.append(row)\n",
    "\n",
    "# Add negatives\n",
    "print(f\"\\n[2/2] Processing {len(all_negatives):,} negative sequences...\")\n",
    "for i, (seq_id, seq) in enumerate(all_negatives.items()):\n",
    "    if i % 300 == 0:\n",
    "        print(f\"  Progress: {i}/{len(all_negatives)}\")\n",
    "    \n",
    "    if 'nonGRNA' in seq_id:\n",
    "        source = 'minicircle_nonGRNA'\n",
    "    elif 'chimeric' in seq_id:\n",
    "        source = 'chimeric'\n",
    "    else:\n",
    "        source = 'random'\n",
    "    \n",
    "    features = extractor.extract_features(seq)\n",
    "    row = {\n",
    "        'sequence_id': seq_id,\n",
    "        'sequence': seq,\n",
    "        'length': len(seq),\n",
    "        'label': 0,\n",
    "        'source': source,\n",
    "        **features\n",
    "    }\n",
    "    data_rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_all = pd.DataFrame(data_rows)\n",
    "\n",
    "# Identify feature columns\n",
    "metadata_cols = ['sequence_id', 'sequence', 'length', 'label', 'source']\n",
    "feature_cols = [c for c in df_all.columns if c not in metadata_cols]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Feature extraction complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüìä Dataset summary:\")\n",
    "print(f\"  Total samples: {len(df_all):,}\")\n",
    "print(f\"  Positive (gRNA): {sum(df_all['label']==1):,}\")\n",
    "print(f\"  Negative: {sum(df_all['label']==0):,}\")\n",
    "print(f\"  Features extracted: {len(feature_cols)}\")\n",
    "\n",
    "# Verify length NOT in features\n",
    "if 'length' in feature_cols:\n",
    "    print(\"\\n  ‚ùå WARNING: 'length' in features - REMOVE IT!\")\n",
    "else:\n",
    "    print(f\"\\n  ‚úì Length correctly EXCLUDED from features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Quality Control\n",
    "\n",
    "Critical checks:\n",
    "1. No NaN/inf values\n",
    "2. Length not in features (would cause leakage)\n",
    "3. Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 5: QUALITY CONTROL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for NaN/inf\n",
    "nan_features = [c for c in feature_cols if df_all[c].isna().any()]\n",
    "inf_features = [c for c in feature_cols if np.isinf(df_all[c]).any()]\n",
    "\n",
    "print(\"\\nüîç Data quality checks:\")\n",
    "print(f\"  Features with NaN: {len(nan_features)}\")\n",
    "print(f\"  Features with inf: {len(inf_features)}\")\n",
    "\n",
    "if nan_features:\n",
    "    print(f\"  ‚ö†Ô∏è Found NaN in: {nan_features[:5]}...\")\n",
    "if inf_features:\n",
    "    print(f\"  ‚ö†Ô∏è Found inf in: {inf_features[:5]}...\")\n",
    "\n",
    "# Check length not in features\n",
    "if 'length' in feature_cols:\n",
    "    print(\"\\n  ‚ùå ERROR: 'length' is in feature columns!\")\n",
    "    print(\"     This will cause the model to learn length artifacts!\")\n",
    "else:\n",
    "    print(\"\\n  ‚úÖ Length correctly excluded from features\")\n",
    "\n",
    "# Class balance\n",
    "n_pos = sum(df_all['label'] == 1)\n",
    "n_neg = sum(df_all['label'] == 0)\n",
    "balance_ratio = min(n_pos, n_neg) / max(n_pos, n_neg)\n",
    "\n",
    "print(f\"\\nüìä Class distribution:\")\n",
    "print(f\"  Positive: {n_pos:,} ({n_pos/len(df_all)*100:.1f}%)\")\n",
    "print(f\"  Negative: {n_neg:,} ({n_neg/len(df_all)*100:.1f}%)\")\n",
    "print(f\"  Balance ratio: {balance_ratio:.3f}\")\n",
    "\n",
    "if balance_ratio < 0.9:\n",
    "    print(\"  ‚ö†Ô∏è Classes are imbalanced!\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Classes are balanced\")\n",
    "\n",
    "print(\"\\n‚úÖ Quality control complete!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 6: Train/Val/Test Split\n",
    "\n",
    "70/15/15 split with stratification by source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 6: TRAIN/VAL/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create stratification column\n",
    "df_all['strat_group'] = df_all['label'].astype(str) + '_' + df_all['source']\n",
    "\n",
    "# First split: 70% train, 30% temp\n",
    "print(\"\\n[1/2] Splitting train vs temp...\")\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_all,\n",
    "    test_size=0.30,\n",
    "    stratify=df_all['strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Temp:  {len(temp_df):,}\")\n",
    "\n",
    "# Second split: 50/50 of temp ‚Üí val and test\n",
    "print(\"\\n[2/2] Splitting val vs test...\")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_df['strat_group'],\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"  Val:   {len(val_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nüìä Final split:\")\n",
    "total = len(df_all)\n",
    "print(f\"  Train: {len(train_df)/total*100:.1f}%\")\n",
    "print(f\"  Val:   {len(val_df)/total*100:.1f}%\")\n",
    "print(f\"  Test:  {len(test_df)/total*100:.1f}%\")\n",
    "\n",
    "# Class balance\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    pos = sum(df['label']==1)\n",
    "    neg = sum(df['label']==0)\n",
    "    print(f\"\\n  {name}: pos={pos:,} neg={neg:,} balance={min(pos,neg)/max(pos,neg):.3f}\")\n",
    "\n",
    "# Drop stratification column\n",
    "train_df = train_df.drop('strat_group', axis=1)\n",
    "val_df = val_df.drop('strat_group', axis=1)\n",
    "test_df = test_df.drop('strat_group', axis=1)\n",
    "\n",
    "print(\"\\n‚úÖ Splitting complete!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 7: Export Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 7: EXPORT DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save CSV files\n",
    "print(\"\\nSaving datasets...\")\n",
    "\n",
    "train_file = PROCESSED_DIR / 'train_data.csv'\n",
    "val_file = PROCESSED_DIR / 'val_data.csv'\n",
    "test_file = PROCESSED_DIR / 'test_data.csv'\n",
    "\n",
    "train_df.to_csv(train_file, index=False)\n",
    "print(f\"  ‚úì {train_file}\")\n",
    "\n",
    "val_df.to_csv(val_file, index=False)\n",
    "print(f\"  ‚úì {val_file}\")\n",
    "\n",
    "test_df.to_csv(test_file, index=False)\n",
    "print(f\"  ‚úì {test_file}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_file = PROCESSED_DIR / 'feature_names.txt'\n",
    "with open(feature_file, 'w') as f:\n",
    "    for feat in feature_cols:\n",
    "        f.write(feat + '\\n')\n",
    "print(f\"  ‚úì {feature_file}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'creation_date': pd.Timestamp.now().isoformat(),\n",
    "    'notebook': '2_data_preparation_comprehensive.ipynb',\n",
    "    'source_files': {\n",
    "        'positive': str(GRNA_FILE),\n",
    "        'minicircle': str(MINICIRCLE_FILE),\n",
    "        'gtf': str(GTF_FILE)\n",
    "    },\n",
    "    'negative_strategy': {\n",
    "        'minicircle_nonGRNA': f'{n_minicircle} (50%)',\n",
    "        'chimeric': f'{n_chimeric} (30%)',\n",
    "        'random': f'{n_random} (20%)'\n",
    "    },\n",
    "    'total_samples': len(df_all),\n",
    "    'n_features': len(feature_cols),\n",
    "    'n_positives': int(sum(df_all['label']==1)),\n",
    "    'n_negatives': int(sum(df_all['label']==0)),\n",
    "    'splits': {\n",
    "        'train': len(train_df),\n",
    "        'val': len(val_df),\n",
    "        'test': len(test_df)\n",
    "    },\n",
    "    'quality_checks': {\n",
    "        'length_excluded': 'length' not in feature_cols,\n",
    "        'ks_test_pval': float(ks_pval),\n",
    "        'class_balance': float(balance_ratio),\n",
    "        'no_nan': len(nan_features) == 0,\n",
    "        'no_inf': len(inf_features) == 0\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = PROCESSED_DIR / 'dataset_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"  ‚úì {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DATA PREPARATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìÅ Output files:\")\n",
    "for f in [train_file, val_file, test_file, feature_file, summary_file]:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"  Total: {len(df_all):,} samples\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Val: {len(val_df):,}\")\n",
    "print(f\"  Test: {len(test_df):,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for training!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
